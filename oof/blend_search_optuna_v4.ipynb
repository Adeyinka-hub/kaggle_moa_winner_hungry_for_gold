{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T16:07:47.821830Z",
     "iopub.status.busy": "2020-11-22T16:07:47.821023Z",
     "iopub.status.idle": "2020-11-22T16:07:49.329586Z",
     "shell.execute_reply": "2020-11-22T16:07:49.328896Z"
    },
    "papermill": {
     "duration": 1.531146,
     "end_time": "2020-11-22T16:07:49.329729",
     "exception": false,
     "start_time": "2020-11-22T16:07:47.798583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[V4]\n",
    "Blend Models:\n",
    "* kibuna-nn-hs-1024-last-train (aka. 2stage-NN, LB: 0.01822)\n",
    "* simple-nn-using-old-cv (LB: 0.01836)\n",
    "* fork-of-2heads-looper-super-puper-markpeng-infer (LB: 0.1836)\n",
    "* deepinsight-efficientnet-v7-b3-infer (LB: 0.01850)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "kernel_mode = False\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import datetime\n",
    "import glob\n",
    "\n",
    "from numba import njit\n",
    "from scipy.optimize import minimize, fsolve\n",
    "\n",
    "import optuna\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "rand_seed = 1120\n",
    "\n",
    "search_mode = True\n",
    "\n",
    "method = \"scipy\"\n",
    "\n",
    "# method = \"optuna\"\n",
    "study_name = \"moa_blend_team_v4\"\n",
    "# n_trials = 500\n",
    "n_trials = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T16:07:49.374396Z",
     "iopub.status.busy": "2020-11-22T16:07:49.372601Z",
     "iopub.status.idle": "2020-11-22T16:07:57.173665Z",
     "shell.execute_reply": "2020-11-22T16:07:57.167043Z"
    },
    "papermill": {
     "duration": 7.831926,
     "end_time": "2020-11-22T16:07:57.173865",
     "exception": false,
     "start_time": "2020-11-22T16:07:49.341939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mkdir -p /root/.cache/torch/hub/checkpoints/\n",
    "# !cp ../input/gen-efficientnet-pretrained/tf_efficientnet_*.pth /root/.cache/torch/hub/checkpoints/\n",
    "# !cp ../input/deepinsight-resnest-v1-resnest50/*.pth /root/.cache/torch/hub/checkpoints/\n",
    "# !cp ../input/deepinsight-resnest-v2-resnest50-output/*.pth /root/.cache/torch/hub/checkpoints/\n",
    "# !ls -la /root/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T16:07:57.302713Z",
     "iopub.status.busy": "2020-11-22T16:07:57.301888Z",
     "iopub.status.idle": "2020-11-22T16:07:59.432020Z",
     "shell.execute_reply": "2020-11-22T16:07:59.427068Z"
    },
    "papermill": {
     "duration": 2.211524,
     "end_time": "2020-11-22T16:07:59.432178",
     "exception": false,
     "start_time": "2020-11-22T16:07:57.220654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !cp ../input/kaggle-moa-team/scripts/* .\n",
    "# !ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T16:07:59.482699Z",
     "iopub.status.busy": "2020-11-22T16:07:59.481671Z",
     "iopub.status.idle": "2020-11-22T16:07:59.486348Z",
     "shell.execute_reply": "2020-11-22T16:07:59.487900Z"
    },
    "papermill": {
     "duration": 0.034902,
     "end_time": "2020-11-22T16:07:59.488095",
     "exception": false,
     "start_time": "2020-11-22T16:07:59.453193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_folder = \"../input/lish-moa\" if kernel_mode else \"/workspace/Kaggle/MoA/\"\n",
    "\n",
    "# Add your model inference script here\n",
    "# Tuple Format: (script, oof_filename, output_filename, weight)\n",
    "model_list = [\n",
    "    (\"../../Github/kaggle_moa_team/scripts/2stageNN_with_ns_oldcv.py\",\n",
    "     \"../../Github/kaggle_moa_team/oof/oof_2stageNN_ns_oldcv.npy\",\n",
    "     \"submission_2stageNN_with_ns_oldcv_0.01822.csv\", 0.25),\n",
    "    \n",
    "    (\"../../Github/kaggle_moa_team/scripts/script_simpleNN_oldcv.py\",\n",
    "     \"../../Github/kaggle_moa_team/oof/oof_script_simpleNN_oldcv.npy\",\n",
    "     \"submission_script_simpleNN_oldcv_0.01836.csv\", 0.25),\n",
    "    \n",
    "    (\"../../Github/kaggle_moa_team/scripts/fork-of-2heads-looper-super-puper-markpeng-infer.py\",\n",
    "     \"../../Github/kaggle_moa_team/oof/oof_fork-of-2heads-looper-super-puper-markpeng.npy\",\n",
    "     \"submission_2heads-looper-super-puper_0.01836.csv\", 0.25),\n",
    "    \n",
    "    (\"../../Github/kaggle_moa_team/scripts/deepinsight_efficientnet_lightning_v7_b3_infer.py\",\n",
    "     \"../../Github/kaggle_moa_team/oof/oof_deepinsight_efficientnet_lightning_v7_b3_0.01850.npy\",\n",
    "     \"submission_effnet_v7_b3_0.01850.csv\", 0.25),\n",
    "    \n",
    "#     (\"deepinsight_resnest_lightning_v2_infer.py\",\n",
    "#      \"oof_deepinsight_ResNeSt_v2_resnest50_0.01455961217985703.npy\",\n",
    "#      \"submission_resnest_v2.csv\", 0),\n",
    "]\n",
    "\n",
    "model_path = \".\" if kernel_mode else dataset_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T16:07:59.535457Z",
     "iopub.status.busy": "2020-11-22T16:07:59.534643Z",
     "iopub.status.idle": "2020-11-22T16:08:00.303180Z",
     "shell.execute_reply": "2020-11-22T16:08:00.303752Z"
    },
    "papermill": {
     "duration": 0.797221,
     "end_time": "2020-11-22T16:08:00.303937",
     "exception": false,
     "start_time": "2020-11-22T16:07:59.506716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(f\"{dataset_folder}/train_features.csv\",\n",
    "                             engine='c')\n",
    "train_labels = pd.read_csv(f'{dataset_folder}/train_targets_scored.csv',\n",
    "                           engine='c')\n",
    "train_classes = [c for c in train_labels.columns if c != \"sig_id\"]\n",
    "\n",
    "non_control_group_rows = train_features[\"cp_type\"] == \"trt_cp\"\n",
    "non_control_group_train_labels = train_labels.loc[\n",
    "    non_control_group_rows, :].copy().reset_index(drop=True)\n",
    "\n",
    "submission = pd.read_csv(f'{dataset_folder}/sample_submission.csv')\n",
    "submission.iloc[:, 1:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T16:08:00.340378Z",
     "iopub.status.busy": "2020-11-22T16:08:00.338486Z",
     "iopub.status.idle": "2020-11-22T16:08:00.341900Z",
     "shell.execute_reply": "2020-11-22T16:08:00.342449Z"
    },
    "papermill": {
     "duration": 0.02445,
     "end_time": "2020-11-22T16:08:00.342614",
     "exception": false,
     "start_time": "2020-11-22T16:08:00.318164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_logloss(y_pred, y_true):\n",
    "    logloss = (1 - y_true) * np.log(1 - y_pred +\n",
    "                                    1e-15) + y_true * np.log(y_pred + 1e-15)\n",
    "    return np.nanmean(-logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_scripts(submission, weights=None):\n",
    "    for i, (script, oof_filename, output_filename, weight) in enumerate(model_list):\n",
    "        print(f\"Generating submission file from {script} ......\")\n",
    "        infer_start = time.time()\n",
    "        !python {model_path}/{script}\n",
    "        infer_elapsed = time.time() - infer_start\n",
    "        print(f\"Time spent on inference: {infer_elapsed/60:.2f} minutes.\")\n",
    "\n",
    "        model_submit = pd.read_csv(output_filename, engine='c')\n",
    "        print(model_submit.head(5))\n",
    "        print(model_submit.shape)\n",
    "        if weights is None:\n",
    "            print(f\"Blending {script} with weight: {weight} ......\")\n",
    "            submission.iloc[:, 1:] += weight * model_submit.iloc[:, 1:]\n",
    "        else:\n",
    "            print(f\"Blending {script} with weight: {weights[i]} ......\")\n",
    "            submission.iloc[:, 1:] += weights[i] * model_submit.iloc[:, 1:]\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T16:08:00.401604Z",
     "iopub.status.busy": "2020-11-22T16:08:00.384877Z",
     "iopub.status.idle": "2020-11-22T16:30:58.239825Z",
     "shell.execute_reply": "2020-11-22T16:30:58.238616Z"
    },
    "papermill": {
     "duration": 1377.882267,
     "end_time": "2020-11-22T16:30:58.240001",
     "exception": false,
     "start_time": "2020-11-22T16:08:00.357734",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 21948, 206)\n",
      "Loading OOF from ../../Github/kaggle_moa_team/oof/oof_2stageNN_ns_oldcv.npy ......\n",
      "OOF Validation Loss of ../../Github/kaggle_moa_team/scripts/2stageNN_with_ns_oldcv.py: 0.015606\n",
      "\n",
      "Loading OOF from ../../Github/kaggle_moa_team/oof/oof_script_simpleNN_oldcv.npy ......\n",
      "OOF Validation Loss of ../../Github/kaggle_moa_team/scripts/script_simpleNN_oldcv.py: 0.015846\n",
      "\n",
      "Loading OOF from ../../Github/kaggle_moa_team/oof/oof_fork-of-2heads-looper-super-puper-markpeng.npy ......\n",
      "OOF Validation Loss of ../../Github/kaggle_moa_team/scripts/fork-of-2heads-looper-super-puper-markpeng-infer.py: 0.015887\n",
      "\n",
      "Loading OOF from oof_deepinsight_efficientnet_v7_b3_0.014802440208660929.npy ......\n",
      "OOF Validation Loss of deepinsight_efficientnet_lightning_v7_b3_infer.py: 0.016016\n",
      "\n",
      "Inital Blend OOF: 0.015370944555914716\n",
      "[00:23] Optimised Blend OOF: 0.015331777381591449\n",
      "Optimised Weights: [0.44090106 0.14508641 0.05945655 0.35455598]\n",
      "Generating submission file from ../../Github/kaggle_moa_team/scripts/2stageNN_with_ns_oldcv.py ......\n",
      "True\n",
      "(23814, 332)\n",
      "(nsamples, nfeatures)\n",
      "(23814, 876)\n",
      "(23814, 207)\n",
      "(23814, 332)\n",
      "(3982, 876)\n",
      "(3982, 207)\n",
      "(23814, 1086)\n",
      "(3982, 1086)\n",
      "(21948, 332)\n",
      "(23814, 1086)\n",
      "(3982, 1086)\n",
      "(21948, 1416)\n",
      "(3624, 1085)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "(21948, 1416)\n",
      "(21948, 1417)\n",
      "(3624, 1085)\n",
      "(21948, 332)\n",
      "(3982, 207)\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/Kaggle/MoA//../../Github/kaggle_moa_team/scripts/2stageNN_with_ns_oldcv.py\", line 674, in <module>\n",
      "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
      "  File \"/workspace/Kaggle/MoA//../../Github/kaggle_moa_team/scripts/2stageNN_with_ns_oldcv.py\", line 655, in run_k_fold\n",
      "    oof_, pred_ = run_training(fold, seed)\n",
      "  File \"/workspace/Kaggle/MoA//../../Github/kaggle_moa_team/scripts/2stageNN_with_ns_oldcv.py\", line 634, in run_training\n",
      "    model.load_state_dict(torch.load(f\"{MODEL_DIR}/{NB}-nonscored-SEED{seed}-FOLD{fold}_.pth\"))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/serialization.py\", line 571, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/serialization.py\", line 229, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/serialization.py\", line 210, in __init__\n",
      "    super(_open_file, self).__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/workspace/Kaggle/MoA/kibuna-nn-hs-1024-last-train-markpeng/model/25-nonscored-SEED0-FOLD0_.pth'\n",
      "\u001b[0mTime spent on inference: 1.75 minutes.\n",
      "         sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
      "0  id_0004d9e33                     0.001149                0.001465   \n",
      "1  id_001897cda                     0.000514                0.002112   \n",
      "2  id_002429b5b                     0.000000                0.000000   \n",
      "3  id_00276f245                     0.001224                0.001149   \n",
      "4  id_0027f1083                     0.001674                0.001859   \n",
      "\n",
      "   acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
      "0        0.001759                        0.011808   \n",
      "1        0.001632                        0.004301   \n",
      "2        0.000000                        0.000000   \n",
      "3        0.001648                        0.014157   \n",
      "4        0.002772                        0.014973   \n",
      "\n",
      "   acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
      "0                           0.019730                        0.004797   \n",
      "1                           0.002107                        0.002263   \n",
      "2                           0.000000                        0.000000   \n",
      "3                           0.020908                        0.003767   \n",
      "4                           0.020677                        0.004415   \n",
      "\n",
      "   adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
      "0                    0.002614                       0.006231   \n",
      "1                    0.006319                       0.012393   \n",
      "2                    0.000000                       0.000000   \n",
      "3                    0.003095                       0.003870   \n",
      "4                    0.003962                       0.003042   \n",
      "\n",
      "   adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n",
      "0                    0.000631  ...                               0.000904   \n",
      "1                    0.005252  ...                               0.001019   \n",
      "2                    0.000000  ...                               0.000000   \n",
      "3                    0.000582  ...                               0.000742   \n",
      "4                    0.000745  ...                               0.001228   \n",
      "\n",
      "   trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n",
      "0      0.001267         0.002736           0.001985   \n",
      "1      0.000657         0.006524           0.000494   \n",
      "2      0.000000         0.000000           0.000000   \n",
      "3      0.001054         0.004266           0.003588   \n",
      "4      0.000941         0.004282           0.003141   \n",
      "\n",
      "   tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n",
      "0                   0.001602                               0.000928   \n",
      "1                   0.009789                               0.000507   \n",
      "2                   0.000000                               0.000000   \n",
      "3                   0.007000                               0.000887   \n",
      "4                   0.002014                               0.000936   \n",
      "\n",
      "   vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n",
      "0         0.002326   0.001742                    0.002772       0.001710  \n",
      "1         0.003183   0.000933                    0.001540       0.002399  \n",
      "2         0.000000   0.000000                    0.000000       0.000000  \n",
      "3         0.002099   0.002357                    0.000819       0.003220  \n",
      "4         0.001957   0.001982                    0.000786       0.001158  \n",
      "\n",
      "[5 rows x 207 columns]\n",
      "(3982, 207)\n",
      "Blending ../../Github/kaggle_moa_team/scripts/2stageNN_with_ns_oldcv.py with weight: 0.44090105711027344 ......\n",
      "Generating submission file from ../../Github/kaggle_moa_team/scripts/script_simpleNN_oldcv.py ......\n",
      "(21948, 1295)\n",
      "(21948, 1296)\n",
      "(3624, 1089)\n",
      "(21948, 207)\n",
      "(3982, 207)\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/Kaggle/MoA//../../Github/kaggle_moa_team/scripts/script_simpleNN_oldcv.py\", line 679, in <module>\n",
      "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
      "  File \"/workspace/Kaggle/MoA//../../Github/kaggle_moa_team/scripts/script_simpleNN_oldcv.py\", line 660, in run_k_fold\n",
      "    oof_, pred_ = run_training(fold, seed)\n",
      "  File \"/workspace/Kaggle/MoA//../../Github/kaggle_moa_team/scripts/script_simpleNN_oldcv.py\", line 639, in run_training\n",
      "    model.load_state_dict(torch.load(f\"../input/simple-nn-using-old-cv/SEED{seed}_FOLD{fold}_.pth\"))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/serialization.py\", line 571, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/serialization.py\", line 229, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/serialization.py\", line 210, in __init__\n",
      "    super(_open_file, self).__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../input/simple-nn-using-old-cv/SEED940_FOLD0_.pth'\n",
      "Time spent on inference: 0.39 minutes.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'submission_script_simpleNN_oldcv_0.01836.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6bdc6dd7ad42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Optimised Weights:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_scipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_inference_scripts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmission\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_scipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-ba4f02db143b>\u001b[0m in \u001b[0;36mrun_inference_scripts\u001b[0;34m(submission, weights)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time spent on inference: {infer_elapsed/60:.2f} minutes.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmodel_submit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_submit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_submit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'submission_script_simpleNN_oldcv_0.01836.csv'"
     ]
    }
   ],
   "source": [
    "total_start = time.time()\n",
    "\n",
    "if not search_mode:\n",
    "    submission = run_inference_scripts(submission)\n",
    "elif search_mode and method == \"optuna\":\n",
    "    ## Search Best Blend Weights by Optuna ##\n",
    "    model_oofs = []\n",
    "\n",
    "    for i, (script, oof_filename, output_filename,\n",
    "            weight) in enumerate(model_list):\n",
    "        print(f\"Loading OOF from {oof_filename} ......\")\n",
    "        oof = np.load(f\"{dataset_folder}/{oof_filename}\")\n",
    "\n",
    "        if oof.shape[0] == 23814:\n",
    "            oof = oof[non_control_group_rows, :]\n",
    "\n",
    "        oof_loss = mean_logloss(\n",
    "            oof, non_control_group_train_labels[train_classes].values)\n",
    "        print(f\"OOF Validation Loss of {script}: {oof_loss:.6f}\\n\")\n",
    "        model_oofs.append(oof)\n",
    "\n",
    "    def objective(trial):\n",
    "        weights = []\n",
    "        for i in range(len(model_list)):\n",
    "            weights.append(trial.suggest_float(f\"w{i}\", 0, 1.0))\n",
    "\n",
    "        blend = np.zeros(model_oofs[0].shape)\n",
    "        for i in range(len(model_list)):\n",
    "            blend += weights[i] * model_oofs[i]\n",
    "        blend = np.clip(blend, 0, 1.0)\n",
    "\n",
    "        loss = mean_logloss(\n",
    "            blend, non_control_group_train_labels[train_classes].values)\n",
    "        return loss\n",
    "\n",
    "    pruner = optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=5,\n",
    "        n_warmup_steps=0,\n",
    "        interval_steps=1,\n",
    "    )\n",
    "    sampler = optuna.samplers.TPESampler(seed=rand_seed)\n",
    "    study = optuna.create_study(direction=\"minimize\",\n",
    "                                pruner=pruner,\n",
    "                                sampler=sampler,\n",
    "                                study_name=study_name,\n",
    "                                storage=f'sqlite:///{study_name}.db',\n",
    "                                load_if_exists=True)\n",
    "\n",
    "    study.optimize(objective,\n",
    "                   n_trials=n_trials,\n",
    "                   timeout=None,\n",
    "                   gc_after_trial=True,\n",
    "                   n_jobs=-1)\n",
    "\n",
    "    trial = study.best_trial\n",
    "\n",
    "    optimal_weights = []\n",
    "    for i, (script, oof_filename, output_filename, _) in enumerate(model_list):\n",
    "        optimal_weights.append(trial.params[f\"w{i}\"])\n",
    "    submission = run_inference_scripts(submission, weights=optimal_weights)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "    print(\"Best trial:\")\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "elif search_mode and method == \"scipy\":\n",
    "    # Optimise Blending Weights with Bonus\n",
    "    # https://www.kaggle.com/gogo827jz/optimise-blending-weights-with-bonus-0/notebook\n",
    "    model_oofs = []\n",
    "    y_true = non_control_group_train_labels[train_classes].values\n",
    "\n",
    "    all_oof = np.zeros(\n",
    "        (len(model_list), non_control_group_train_labels.shape[0], 206))\n",
    "    print(all_oof.shape)\n",
    "    for i, (script, oof_filename, output_filename,\n",
    "            weight) in enumerate(model_list):\n",
    "        print(f\"Loading OOF from {oof_filename} ......\")\n",
    "        oof = np.load(f\"{dataset_folder}/{oof_filename}\")\n",
    "\n",
    "        if oof.shape[0] == 23814:\n",
    "            oof = oof[non_control_group_rows, :]\n",
    "\n",
    "        all_oof[i, :, :] = oof\n",
    "\n",
    "        oof_loss = mean_logloss(oof, y_true)\n",
    "        print(f\"OOF Validation Loss of {script}: {oof_loss:.6f}\\n\")\n",
    "        model_oofs.append(oof)\n",
    "\n",
    "    # Reference: https://www.kaggle.com/gogo827jz/optimise-blending-weights-with-bonus-0/notebook\n",
    "    # CPMP's logloss from https://www.kaggle.com/c/lish-moa/discussion/183010\n",
    "    def log_loss_numpy(y_pred, y_true):\n",
    "        y_true_ravel = np.asarray(y_true).ravel()\n",
    "        y_pred = np.asarray(y_pred).ravel()\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        loss = np.where(y_true_ravel == 1, -np.log(y_pred),\n",
    "                        -np.log(1 - y_pred))\n",
    "        return loss.mean()\n",
    "\n",
    "    def func_numpy_metric(weights):\n",
    "        oof_blend = np.tensordot(weights, all_oof, axes=((0), (0)))\n",
    "        return log_loss_numpy(oof_blend, y_true)\n",
    "\n",
    "    @njit\n",
    "    def grad_func_jit(weights):\n",
    "        oof_clip = np.minimum(1 - 1e-15, np.maximum(all_oof, 1e-15))\n",
    "        gradients = np.zeros(all_oof.shape[0])\n",
    "        for i in range(all_oof.shape[0]):\n",
    "            a, b, c = y_true, oof_clip[i], np.zeros(\n",
    "                (all_oof.shape[1], all_oof.shape[2]))\n",
    "            for j in range(oof.shape[0]):\n",
    "                if j != i:\n",
    "                    c += weights[j] * oof_clip[j]\n",
    "            gradients[i] = -np.mean(\n",
    "                (-a * b + (b**2) * weights[i] + b * c) /\n",
    "                ((b**2) *\n",
    "                 (weights[i]**2) + 2 * b * c * weights[i] - b * weights[i] +\n",
    "                 (c**2) - c))\n",
    "        return gradients\n",
    "\n",
    "    tol = 1e-10\n",
    "    init_guess = [1 / all_oof.shape[0]] * all_oof.shape[0]\n",
    "    bnds = [(0, 1) for _ in range(all_oof.shape[0])]\n",
    "    cons = {\n",
    "        'type': 'eq',\n",
    "        'fun': lambda x: np.sum(x) - 1,\n",
    "        'jac': lambda x: [1] * len(x)\n",
    "    }\n",
    "\n",
    "    print('Inital Blend OOF:', func_numpy_metric(init_guess))\n",
    "\n",
    "    start_time = time.time()\n",
    "    res_scipy = minimize(\n",
    "        fun=func_numpy_metric,\n",
    "        x0=init_guess,\n",
    "        method='SLSQP',\n",
    "        # jac=grad_func_jit,  # grad_func\n",
    "        bounds=bnds,\n",
    "        constraints=cons,\n",
    "        tol=tol)\n",
    "    print(\n",
    "        f'[{str(datetime.timedelta(seconds = time.time() - start_time))[2:7]}] Optimised Blend OOF:',\n",
    "        res_scipy.fun)\n",
    "    print('Optimised Weights:', res_scipy.x)\n",
    "\n",
    "    submission = run_inference_scripts(submission, weights=res_scipy.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T16:30:58.766078Z",
     "iopub.status.busy": "2020-11-22T16:30:58.764325Z",
     "iopub.status.idle": "2020-11-22T16:30:58.782502Z",
     "shell.execute_reply": "2020-11-22T16:30:58.781733Z"
    },
    "papermill": {
     "duration": 0.268265,
     "end_time": "2020-11-22T16:30:58.782670",
     "exception": false,
     "start_time": "2020-11-22T16:30:58.514405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_elapsed = time.time() - total_start\n",
    "print(f\"Total time spent: {total_elapsed/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [V3]\n",
    "# improving-mark-s-2-heads-model-infer\n",
    "# Number of finished trials: 3000\n",
    "# Best trial:\n",
    "#   Value: 0.01515466145873492\n",
    "#   Params: \n",
    "#     w0: 0.0002980690037490555\n",
    "#     w1: 0.29771381784976886\n",
    "#     w2: 0.1569191862042946\n",
    "#     w3: 0.18156875605872544\n",
    "#     w4: 0.36371774630338105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [V3]\n",
    "# fork-of-2heads-looper-super-puper-markpeng-infer\n",
    "# Number of finished trials: 3000\n",
    "# Best trial:\n",
    "#   Value: 0.015170138066049686\n",
    "#   Params: \n",
    "#     w0: 0.00019903389488299251\n",
    "#     w1: 0.3853752127955825\n",
    "#     w2: 0.015968332256452233\n",
    "#     w3: 0.22945916769823432\n",
    "#     w4: 0.3711290150522236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T16:31:00.729455Z",
     "iopub.status.busy": "2020-11-22T16:31:00.728462Z",
     "iopub.status.idle": "2020-11-22T16:31:00.766500Z",
     "shell.execute_reply": "2020-11-22T16:31:00.767046Z"
    },
    "papermill": {
     "duration": 0.294364,
     "end_time": "2020-11-22T16:31:00.767175",
     "exception": false,
     "start_time": "2020-11-22T16:31:00.472811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(submission.shape)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T16:31:01.305669Z",
     "iopub.status.busy": "2020-11-22T16:31:01.304756Z",
     "iopub.status.idle": "2020-11-22T16:31:03.397179Z",
     "shell.execute_reply": "2020-11-22T16:31:03.396062Z"
    },
    "papermill": {
     "duration": 2.337272,
     "end_time": "2020-11-22T16:31:03.397306",
     "exception": false,
     "start_time": "2020-11-22T16:31:01.060034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.239792,
     "end_time": "2020-11-22T16:31:03.876454",
     "exception": false,
     "start_time": "2020-11-22T16:31:03.636662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T16:31:04.372710Z",
     "iopub.status.busy": "2020-11-22T16:31:04.371600Z",
     "iopub.status.idle": "2020-11-22T16:31:05.717376Z",
     "shell.execute_reply": "2020-11-22T16:31:05.716504Z"
    },
    "papermill": {
     "duration": 1.595866,
     "end_time": "2020-11-22T16:31:05.717490",
     "exception": false,
     "start_time": "2020-11-22T16:31:04.121624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if kernel_mode:\n",
    "    !rm ./*.py\n",
    "    !ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.243405,
     "end_time": "2020-11-22T16:31:06.199770",
     "exception": false,
     "start_time": "2020-11-22T16:31:05.956365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "papermill": {
   "duration": 1402.82753,
   "end_time": "2020-11-22T16:31:06.568888",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-22T16:07:43.741358",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
